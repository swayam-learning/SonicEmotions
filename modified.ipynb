{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper to scrape Comments from subreddits related to mental health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Database: mentalhealthdb\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "from streamlit_app.db_config import MYSQL_DATABASE,MYSQL_HOST,MYSQL_PASSWORD,MYSQL_USER\n",
    "try:\n",
    "    # Establish connection\n",
    "    mydb = pymysql.connect(\n",
    "        host=MYSQL_HOST,\n",
    "        user=MYSQL_USER,\n",
    "        password=MYSQL_PASSWORD,\n",
    "        database=MYSQL_DATABASE\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    \n",
    "    # Verify connection\n",
    "    mycursor.execute(\"SELECT DATABASE()\")\n",
    "    current_db = mycursor.fetchone()\n",
    "    print(f\"âœ… Connected to Database: {current_db[0]}\")\n",
    "\n",
    "except pymysql.MySQLError as err:\n",
    "    print(\"âŒ Error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tables: (('reddit_comments',), ('reddit_posts',))\n"
     ]
    }
   ],
   "source": [
    "mycursor.execute(\"SHOW TABLES\")\n",
    "tables = mycursor.fetchall()\n",
    "print(\"Existing tables:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pymysql\n",
    "from streamlit_app.db_config import MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"reddit_client_id\")\n",
    "client_secret = os.getenv(\"reddit_client_secret\")\n",
    "user_agent = os.getenv(\"reddit_user_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_db():\n",
    "    \"\"\"Establishes a connection to the MySQL database.\"\"\"\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DATABASE,\n",
    "            charset='utf8mb4',\n",
    "            cursorclass=pymysql.cursors.DictCursor\n",
    "        )\n",
    "        return conn\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(\"âŒ Database Connection Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Scraping new posts...\n",
      "ðŸ“Œ Stored 100 posts so far...\n",
      "ðŸ“Œ Scraped 100 unique posts so far from new...\n",
      "ðŸ“Œ Stored 200 posts so far...\n",
      "ðŸ“Œ Scraped 200 unique posts so far from new...\n",
      "ðŸ“Œ Stored 300 posts so far...\n",
      "ðŸ“Œ Scraped 300 unique posts so far from new...\n",
      "ðŸ“Œ Stored 400 posts so far...\n",
      "ðŸ“Œ Scraped 400 unique posts so far from new...\n",
      "ðŸ“Œ Stored 500 posts so far...\n",
      "ðŸ“Œ Scraped 500 unique posts so far from new...\n",
      "ðŸ“Œ Stored 600 posts so far...\n",
      "ðŸ“Œ Scraped 600 unique posts so far from new...\n",
      "ðŸ“Œ Stored 700 posts so far...\n",
      "ðŸ“Œ Scraped 700 unique posts so far from new...\n",
      "ðŸ“Œ Stored 800 posts so far...\n",
      "ðŸ“Œ Scraped 800 unique posts so far from new...\n",
      "ðŸ“Œ Stored 900 posts so far...\n",
      "ðŸ“Œ Scraped 900 unique posts so far from new...\n",
      "ðŸ“Œ Scraped 975 unique posts so far from new...\n",
      "ðŸ“Œ No more posts in new\n",
      "ðŸ“Œ Scraping hot posts...\n",
      "ðŸ“Œ Stored 1000 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1100 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1200 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1300 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1400 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1500 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1600 posts so far...\n",
      "ðŸ“Œ Scraped 977 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1700 posts so far...\n",
      "ðŸ“Œ Scraped 981 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1800 posts so far...\n",
      "ðŸ“Œ Scraped 986 unique posts so far from hot...\n",
      "ðŸ“Œ Stored 1900 posts so far...\n",
      "ðŸ“Œ Scraped 1011 unique posts so far from hot...\n",
      "ðŸ“Œ No more posts in hot\n",
      "ðŸ“Œ Scraping top_all posts...\n",
      "ðŸ“Œ Stored 2000 posts so far...\n",
      "ðŸ“Œ Scraped 1110 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2100 posts so far...\n",
      "ðŸ“Œ Scraped 1210 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2200 posts so far...\n",
      "ðŸ“Œ Scraped 1310 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2300 posts so far...\n",
      "ðŸ“Œ Scraped 1410 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2400 posts so far...\n",
      "ðŸ“Œ Scraped 1510 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2500 posts so far...\n",
      "ðŸ“Œ Scraped 1610 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2600 posts so far...\n",
      "ðŸ“Œ Scraped 1710 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2700 posts so far...\n",
      "ðŸ“Œ Scraped 1810 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2800 posts so far...\n",
      "ðŸ“Œ Scraped 1910 unique posts so far from top_all...\n",
      "ðŸ“Œ Stored 2900 posts so far...\n",
      "ðŸ“Œ Scraped 2002 unique posts so far from top_all...\n",
      "ðŸ“Œ No more posts in top_all\n",
      "ðŸ“Œ Scraping top_year posts...\n",
      "ðŸ“Œ Stored 3000 posts so far...\n",
      "ðŸ“Œ Scraped 2056 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3100 posts so far...\n",
      "ðŸ“Œ Scraped 2156 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3200 posts so far...\n",
      "ðŸ“Œ Scraped 2256 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3300 posts so far...\n",
      "ðŸ“Œ Scraped 2356 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3400 posts so far...\n",
      "ðŸ“Œ Scraped 2456 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3500 posts so far...\n",
      "ðŸ“Œ Scraped 2556 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3600 posts so far...\n",
      "ðŸ“Œ Scraped 2656 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3700 posts so far...\n",
      "ðŸ“Œ Scraped 2756 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3800 posts so far...\n",
      "ðŸ“Œ Scraped 2856 unique posts so far from top_year...\n",
      "ðŸ“Œ Stored 3900 posts so far...\n",
      "ðŸ“Œ Scraped 2914 unique posts so far from top_year...\n",
      "ðŸ“Œ No more posts in top_year\n",
      "ðŸ“Œ Scraped a total of 2914 unique posts from r/mentalhealth\n",
      "âœ… Stored 3916 posts in MySQL!\n"
     ]
    }
   ],
   "source": [
    "def scrape_posts(subreddit_name=\"suicide\", target=10000):\n",
    "    \"\"\"Scrapes posts from a subreddit across multiple listing types.\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = set()  # Use a set to avoid duplicates by post_id\n",
    "    listing_types = [\n",
    "        (\"new\", subreddit.new),\n",
    "        (\"hot\", subreddit.hot),\n",
    "        (\"top_all\", lambda limit, params: subreddit.top(time_filter=\"all\", limit=limit, params=params)),\n",
    "        (\"top_year\", lambda limit, params: subreddit.top(time_filter=\"year\", limit=limit, params=params))\n",
    "    ]\n",
    "\n",
    "    for listing_name, listing_func in listing_types:\n",
    "        if len(posts) >= target:\n",
    "            break\n",
    "        print(f\"ðŸ“Œ Scraping {listing_name} posts...\")\n",
    "        after = None\n",
    "        while len(posts) < target:\n",
    "            try:\n",
    "                # Fetch posts (limit=100 per request due to API constraints)\n",
    "                new_posts = listing_func(limit=100, params={'after': after})\n",
    "                new_posts_list = list(new_posts)\n",
    "\n",
    "                if not new_posts_list:\n",
    "                    print(f\"ðŸ“Œ No more posts in {listing_name}\")\n",
    "                    break\n",
    "\n",
    "                for post in new_posts_list:\n",
    "                    post_data = {\n",
    "                        \"post_id\": post.id,\n",
    "                        \"title\": post.title,\n",
    "                        \"body\": post.selftext if post.selftext else \"No text\",\n",
    "                        \"upvotes\": post.score,\n",
    "                        \"created_at\": post.created_utc  # UNIX timestamp\n",
    "                    }\n",
    "                    posts.add(post.id)  # Track unique post IDs\n",
    "                    yield post_data  # Yield each post for immediate processing\n",
    "\n",
    "                after = new_posts_list[-1].fullname\n",
    "                print(f\"ðŸ“Œ Scraped {len(posts)} unique posts so far from {listing_name}...\")\n",
    "                time.sleep(2)  # Respect Reddit's rate limit (30 requests/minute)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error scraping {listing_name}: {e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"ðŸ“Œ Scraped a total of {len(posts)} unique posts from r/{subreddit_name}\")\n",
    "\n",
    "def store_posts(posts):\n",
    "    \"\"\"Inserts scraped posts into the MySQL database.\"\"\"\n",
    "    conn = connect_db()\n",
    "    if not conn:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        count = 0\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO reddit_posts (post_id, title, body, upvotes, created_at)\n",
    "        VALUES (%s, %s, %s, %s, FROM_UNIXTIME(%s))\n",
    "        ON DUPLICATE KEY UPDATE \n",
    "        title = VALUES(title), \n",
    "        body = VALUES(body), \n",
    "        upvotes = VALUES(upvotes), \n",
    "        created_at = VALUES(created_at)\n",
    "        \"\"\"\n",
    "\n",
    "        for post in posts:\n",
    "            cursor.execute(query, (\n",
    "                post[\"post_id\"], post[\"title\"], post[\"body\"], \n",
    "                post[\"upvotes\"], post[\"created_at\"]\n",
    "            ))\n",
    "            count += 1\n",
    "            if count % 100 == 0:  # Commit in batches\n",
    "                conn.commit()\n",
    "                print(f\"ðŸ“Œ Stored {count} posts so far...\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"âœ… Stored {count} posts in MySQL!\")\n",
    "\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(\"âŒ MySQL Error:\", e)\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Run the scraping and storing process\n",
    "if __name__ == \"__main__\":\n",
    "    posts_generator = scrape_posts(subreddit_name=\"mentalhealth\", target=10000)\n",
    "    store_posts(posts_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
