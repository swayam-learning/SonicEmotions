{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper to scrape Comments from subreddits related to mental health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Database: mentalhealthdb\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "from streamlit_app.db_config import MYSQL_DATABASE,MYSQL_HOST,MYSQL_PASSWORD,MYSQL_USER\n",
    "try:\n",
    "    # Establish connection\n",
    "    mydb = pymysql.connect(\n",
    "        host=MYSQL_HOST,\n",
    "        user=MYSQL_USER,\n",
    "        password=MYSQL_PASSWORD,\n",
    "        database=MYSQL_DATABASE\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    \n",
    "    # Verify connection\n",
    "    mycursor.execute(\"SELECT DATABASE()\")\n",
    "    current_db = mycursor.fetchone()\n",
    "    print(f\"✅ Connected to Database: {current_db[0]}\")\n",
    "\n",
    "except pymysql.MySQLError as err:\n",
    "    print(\"❌ Error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing tables: (('reddit_comments',), ('reddit_posts',))\n"
     ]
    }
   ],
   "source": [
    "mycursor.execute(\"SHOW TABLES\")\n",
    "tables = mycursor.fetchall()\n",
    "print(\"Existing tables:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pymysql\n",
    "from streamlit_app.db_config import MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client_id = os.getenv(\"reddit_client_id\")\n",
    "client_secret = os.getenv(\"reddit_client_secret\")\n",
    "user_agent = os.getenv(\"reddit_user_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_db():\n",
    "    \"\"\"Establishes a connection to the MySQL database.\"\"\"\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DATABASE,\n",
    "            charset='utf8mb4',\n",
    "            cursorclass=pymysql.cursors.DictCursor\n",
    "        )\n",
    "        return conn\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(\"❌ Database Connection Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper and Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Scraping new posts...\n",
      "📌 Stored 100 posts so far...\n",
      "📌 Scraped 100 unique posts so far from new...\n",
      "📌 Stored 200 posts so far...\n",
      "📌 Scraped 200 unique posts so far from new...\n",
      "📌 Stored 300 posts so far...\n",
      "📌 Scraped 300 unique posts so far from new...\n",
      "📌 Stored 400 posts so far...\n",
      "📌 Scraped 400 unique posts so far from new...\n",
      "📌 Stored 500 posts so far...\n",
      "📌 Scraped 500 unique posts so far from new...\n",
      "📌 Stored 600 posts so far...\n",
      "📌 Scraped 600 unique posts so far from new...\n",
      "📌 Stored 700 posts so far...\n",
      "📌 Scraped 700 unique posts so far from new...\n",
      "📌 Stored 800 posts so far...\n",
      "📌 Scraped 800 unique posts so far from new...\n",
      "📌 Stored 900 posts so far...\n",
      "📌 Scraped 900 unique posts so far from new...\n",
      "📌 Scraped 975 unique posts so far from new...\n",
      "📌 No more posts in new\n",
      "📌 Scraping hot posts...\n",
      "📌 Stored 1000 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1100 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1200 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1300 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1400 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1500 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1600 posts so far...\n",
      "📌 Scraped 977 unique posts so far from hot...\n",
      "📌 Stored 1700 posts so far...\n",
      "📌 Scraped 981 unique posts so far from hot...\n",
      "📌 Stored 1800 posts so far...\n",
      "📌 Scraped 986 unique posts so far from hot...\n",
      "📌 Stored 1900 posts so far...\n",
      "📌 Scraped 1011 unique posts so far from hot...\n",
      "📌 No more posts in hot\n",
      "📌 Scraping top_all posts...\n",
      "📌 Stored 2000 posts so far...\n",
      "📌 Scraped 1110 unique posts so far from top_all...\n",
      "📌 Stored 2100 posts so far...\n",
      "📌 Scraped 1210 unique posts so far from top_all...\n",
      "📌 Stored 2200 posts so far...\n",
      "📌 Scraped 1310 unique posts so far from top_all...\n",
      "📌 Stored 2300 posts so far...\n",
      "📌 Scraped 1410 unique posts so far from top_all...\n",
      "📌 Stored 2400 posts so far...\n",
      "📌 Scraped 1510 unique posts so far from top_all...\n",
      "📌 Stored 2500 posts so far...\n",
      "📌 Scraped 1610 unique posts so far from top_all...\n",
      "📌 Stored 2600 posts so far...\n",
      "📌 Scraped 1710 unique posts so far from top_all...\n",
      "📌 Stored 2700 posts so far...\n",
      "📌 Scraped 1810 unique posts so far from top_all...\n",
      "📌 Stored 2800 posts so far...\n",
      "📌 Scraped 1910 unique posts so far from top_all...\n",
      "📌 Stored 2900 posts so far...\n",
      "📌 Scraped 2002 unique posts so far from top_all...\n",
      "📌 No more posts in top_all\n",
      "📌 Scraping top_year posts...\n",
      "📌 Stored 3000 posts so far...\n",
      "📌 Scraped 2056 unique posts so far from top_year...\n",
      "📌 Stored 3100 posts so far...\n",
      "📌 Scraped 2156 unique posts so far from top_year...\n",
      "📌 Stored 3200 posts so far...\n",
      "📌 Scraped 2256 unique posts so far from top_year...\n",
      "📌 Stored 3300 posts so far...\n",
      "📌 Scraped 2356 unique posts so far from top_year...\n",
      "📌 Stored 3400 posts so far...\n",
      "📌 Scraped 2456 unique posts so far from top_year...\n",
      "📌 Stored 3500 posts so far...\n",
      "📌 Scraped 2556 unique posts so far from top_year...\n",
      "📌 Stored 3600 posts so far...\n",
      "📌 Scraped 2656 unique posts so far from top_year...\n",
      "📌 Stored 3700 posts so far...\n",
      "📌 Scraped 2756 unique posts so far from top_year...\n",
      "📌 Stored 3800 posts so far...\n",
      "📌 Scraped 2856 unique posts so far from top_year...\n",
      "📌 Stored 3900 posts so far...\n",
      "📌 Scraped 2914 unique posts so far from top_year...\n",
      "📌 No more posts in top_year\n",
      "📌 Scraped a total of 2914 unique posts from r/mentalhealth\n",
      "✅ Stored 3916 posts in MySQL!\n"
     ]
    }
   ],
   "source": [
    "def scrape_posts(subreddit_name=\"suicide\", target=10000):\n",
    "    \"\"\"Scrapes posts from a subreddit across multiple listing types.\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = set()  # Use a set to avoid duplicates by post_id\n",
    "    listing_types = [\n",
    "        (\"new\", subreddit.new),\n",
    "        (\"hot\", subreddit.hot),\n",
    "        (\"top_all\", lambda limit, params: subreddit.top(time_filter=\"all\", limit=limit, params=params)),\n",
    "        (\"top_year\", lambda limit, params: subreddit.top(time_filter=\"year\", limit=limit, params=params))\n",
    "    ]\n",
    "\n",
    "    for listing_name, listing_func in listing_types:\n",
    "        if len(posts) >= target:\n",
    "            break\n",
    "        print(f\"📌 Scraping {listing_name} posts...\")\n",
    "        after = None\n",
    "        while len(posts) < target:\n",
    "            try:\n",
    "                # Fetch posts (limit=100 per request due to API constraints)\n",
    "                new_posts = listing_func(limit=100, params={'after': after})\n",
    "                new_posts_list = list(new_posts)\n",
    "\n",
    "                if not new_posts_list:\n",
    "                    print(f\"📌 No more posts in {listing_name}\")\n",
    "                    break\n",
    "\n",
    "                for post in new_posts_list:\n",
    "                    post_data = {\n",
    "                        \"post_id\": post.id,\n",
    "                        \"title\": post.title,\n",
    "                        \"body\": post.selftext if post.selftext else \"No text\",\n",
    "                        \"upvotes\": post.score,\n",
    "                        \"created_at\": post.created_utc  # UNIX timestamp\n",
    "                    }\n",
    "                    posts.add(post.id)  # Track unique post IDs\n",
    "                    yield post_data  # Yield each post for immediate processing\n",
    "\n",
    "                after = new_posts_list[-1].fullname\n",
    "                print(f\"📌 Scraped {len(posts)} unique posts so far from {listing_name}...\")\n",
    "                time.sleep(2)  # Respect Reddit's rate limit (30 requests/minute)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error scraping {listing_name}: {e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"📌 Scraped a total of {len(posts)} unique posts from r/{subreddit_name}\")\n",
    "\n",
    "def store_posts(posts):\n",
    "    \"\"\"Inserts scraped posts into the MySQL database.\"\"\"\n",
    "    conn = connect_db()\n",
    "    if not conn:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        count = 0\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO reddit_posts (post_id, title, body, upvotes, created_at)\n",
    "        VALUES (%s, %s, %s, %s, FROM_UNIXTIME(%s))\n",
    "        ON DUPLICATE KEY UPDATE \n",
    "        title = VALUES(title), \n",
    "        body = VALUES(body), \n",
    "        upvotes = VALUES(upvotes), \n",
    "        created_at = VALUES(created_at)\n",
    "        \"\"\"\n",
    "\n",
    "        for post in posts:\n",
    "            cursor.execute(query, (\n",
    "                post[\"post_id\"], post[\"title\"], post[\"body\"], \n",
    "                post[\"upvotes\"], post[\"created_at\"]\n",
    "            ))\n",
    "            count += 1\n",
    "            if count % 100 == 0:  # Commit in batches\n",
    "                conn.commit()\n",
    "                print(f\"📌 Stored {count} posts so far...\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"✅ Stored {count} posts in MySQL!\")\n",
    "\n",
    "    except pymysql.MySQLError as e:\n",
    "        print(\"❌ MySQL Error:\", e)\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Run the scraping and storing process\n",
    "if __name__ == \"__main__\":\n",
    "    posts_generator = scrape_posts(subreddit_name=\"mentalhealth\", target=10000)\n",
    "    store_posts(posts_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
